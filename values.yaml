nameOverride: ""
PrometheusRules:
  core-dns-record-rules:
    groups:
    - name: core-dns-record-rules
      rules:
        - record: 'coredns:cache_hits_rate'
          expr: |
            sum by (kubernetes_cluster)((delta(coredns_cache_hits_total{instance!~"node-local-.*"}[1m])))
        - record: 'coredns:cache_hits_rate_zscore'
          expr: |
            (coredns:cache_hits_rate - avg_over_time(coredns:cache_hits_rate[1d:10m]) ) / (stddev_over_time(coredns:cache_hits_rate[1d:10m]))
        - record: 'coredns:requests_rate'
          expr: |
            sum(delta(coredns_dns_request_type_count_total{instance!~"node-local-.*"}[1m])) by (type, kubernetes_cluster)
        - record: 'coredns:requests_rate_zscore'
          expr: |
            (coredns:requests_rate - avg_over_time(coredns:requests_rate[1d:10m]) ) / (stddev_over_time(coredns:requests_rate[1d:10m]))
        - record: 'coredns:error_rate'
          expr: |
            sum(delta(coredns_dns_response_rcode_count_total{instance!~"node-local-.*"}[1m])) by(zone,rcode, kubernetes_cluster)
        - record: 'coredns:error_rate_zscore'
          expr: |
            (coredns:error_rate - avg_over_time(coredns:error_rate[1d:10m]) ) / (stddev_over_time(coredns:error_rate[1d:10m]))
        - record: 'coredns:requests_rate_zscore_abs'
          expr: |
            abs(coredns:cache_requests_rate_zscore)
        - record: 'coredns:error_rate_zscore_abs'
          expr: |
            abs(coredns:cache_error_rate_zscore)
        - record: 'coredns:cache_hits_rate_zscore_abs'
          expr: |
            abs(coredns:cache_hits_rate_zscore)
  k8s-record-rules:
    groups:
    - name: k8s-record-rules
      rules:
      # Node record rules
      - record: kube_node:node_boot_time_seconds_per_instance_name:sum
        expr: |
          sum without (private_ip, public_ip, instance, label_kubernetes_io_instance_type)(node_boot_time_seconds)

      - record: 'node:node_status_pods_capacity:'
        expr: |
          label_replace(kube_node_status_capacity_pods{job=~"kubernetes_.+"},"instance_name", "$1", "node", "(.*)")

      - record: 'node:node_running_pod_count:sum'
        expr: |
          label_replace(kubelet_running_pod_count{job=~"kubernetes_.+"}, "instance_name", "$1", "instance", "(.*)")

      - record: 'node:cpu:sum'
        expr: |
          count by (kubernetes_cluster, instance_name) (node_cpu_seconds_total{job=~"kubernetes_.+", mode="system"})

      - record: 'node:node_allocatable_cpu_cores:sum'
        expr: |
          label_replace(kube_node_status_allocatable_cpu_cores{job=~"kubernetes_.+"}, "instance_name", "$1", "instance", "(.*)")

      - record: 'node:node_container_cpu_usage_seconds:irate1m'
        expr: |
          sum by(kubernetes_cluster, instance_name)(label_replace(irate(container_cpu_usage_seconds_total{job=~"kubernetes_.+",  container!="POD", container!=""}[1m]), "instance_name", "$1", "instance", "(.*)"))

      - record: 'node:node_allocatable_memory_bytes:sum'
        expr: |
          label_replace(kube_node_status_allocatable_memory_bytes{job=~"kubernetes_.+"}, "instance_name", "$1", "node", "(.*)")

      - record: 'node:node_container_memory_wss_bytes:sum'
        expr: |
          sum by(kubernetes_cluster, instance_name)(label_replace(container_memory_working_set_bytes{job=~"kubernetes_.+",  container!="POD", container!=""}, "instance_name", "$1", "instance", "(.*)"))

      - record: 'node:node_filesystem_usage:ratio'
        expr: |
          max by (kubernetes_cluster, instance_name, mountpoint) ((node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"} - node_filesystem_avail_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"}) /  node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"})

      - record: 'node:node_filesystem_avail:ratio'
        expr: |
          max by (kubernetes_cluster, instance_name, mountpoint) (node_filesystem_avail_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"} / node_filesystem_size_bytes{job=~"kubernetes_.+", fstype=~"ext[234]|btrfs|xfs|zfs"})

      - record: 'kubelet:persistent_volume_avail:ratio'
        expr: |
          kubelet_volume_stats_available_bytes{job=~"kubernetes_.+"} / kubelet_volume_stats_capacity_bytes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_usage:ratio'
        expr: |
          kubelet_volume_stats_used_bytes{job=~"kubernetes_.+"} / kubelet_volume_stats_capacity_bytes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_inode_avail:ratio'
        expr: |
          kubelet_volume_stats_inodes_free{job=~"kubernetes_.+"} / kubelet_volume_stats_inodes{job=~"kubernetes_.+"}

      - record: 'kubelet:persistent_volume_inode_usage:ratio'
        expr: |
          kubelet_volume_stats_inodes_used{job=~"kubernetes_.+"} / kubelet_volume_stats_inodes{job=~"kubernetes_.+"}

      # Namespace record rules
      - record: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum'
        expr: |
          sum by(kubernetes_cluster, namespace, pod)(kube_pod_container_resource_requests_cpu_cores{job=~"kubernetes_.+"} * on (kubernetes_cluster, job, instance, namespace, pod) group_left(phase) (kube_pod_status_phase{job=~"kubernetes_.+", phase=~"^(Pending|Running)$"} == 1))

      - record: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum'
        expr: |
          sum by(kubernetes_cluster, namespace, pod)(kube_pod_container_resource_requests_memory_bytes{job=~"kubernetes_.+"} * on (kubernetes_cluster, job, instance, namespace, pod) group_left(phase) (kube_pod_status_phase{job=~"kubernetes_.+", phase=~"^(Pending|Running)$"} == 1))

      # Cest/cet time records
      # https://medium.com/@tom.fawcett/time-of-day-based-notifications-with-prometheus-and-alertmanager-1bf7a23b7695
      - record: is_israel_summer_time
        expr: |
          (vector(1) and (month() > 3 and month() < 10))
          or
          (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 25) and (day_of_week() == 0)))
          or
          vector(0)
      - record: israel_localtime
        expr: time() + 3600 + 3600 + 3600 * is_israel_summer_time
      - record: business_day
        expr: vector(1) and day_of_week(israel_localtime) >= 0 and day_of_week(israel_localtime) < 5
      - record: israel_hour
        expr: hour(israel_localtime)
      - record: business_hour
        expr: vector(1) and israel_hour >= 9 < 20
      - record: warning_time
        expr: business_day and business_hour
      # - record: europe_london_time
      #   expr: time() + 3600 * is_european_summer_time
      # - record: europe_london_hour
      #   expr: hour(europe_london_time)
      # - record: central_europe_time
      #   expr: time() + 3600 + 3600 * is_european_summer_time
      # - record: central_europe_hour
      #   expr: hour(central_europe_time)
      - record: kube_cronjob_last_execution
        expr: |
          label_replace(
            label_replace(
              max(
                max by (job_name,kubernetes_cluster, namespace)(kube_job_status_start_time)
                * ON(job_name,kubernetes_cluster,namespace) GROUP_RIGHT()
                kube_job_labels{label_cron!=""}
              ) BY (job_name, label_cron,kubernetes_cluster,namespace)
              == ON(label_cron,kubernetes_cluster,namespace) GROUP_LEFT()
              max(
                max by (job_name, kubernetes_cluster,namespace)(kube_job_status_start_time)
                * ON(job_name, kubernetes_cluster,namespace) GROUP_RIGHT()
                kube_job_labels{label_cron!=""}
              ) BY (label_cron, kubernetes_cluster,namespace),
              "job", "$1", "job_name", "(.+)"),
            "cronjob", "$1", "label_cron", "(.+)")
      - record: kube_cronjob_last_failed
        expr: |
          clamp_max(
            kube_cronjob_last_execution,
          1)
          * ON(job_name, kubernetes_cluster, namespace) GROUP_LEFT()
          label_replace(
            label_replace(
              (kube_job_status_failed != 0),
              "job", "$1", "job_name", "(.+)"),
            "cronjob", "$1", "label_cron", "(.+)")
      - expr: sum(rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])) by (namespace)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |-
          sum by (cluster, namespace, pod, container) (
            rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}[5m])
          ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - expr: |-
          sum by (cluster, namespace, pod, container) (
            irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
          ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
      - expr: |-
          container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_working_set_bytes
      - expr: |-
          container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_rss
      - expr: |-
          container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_cache
      - expr: |-
          container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
            max by(namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_swap
      - expr: sum(container_memory_usage_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!="", container!="POD"}) by (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: |-
          sum by (namespace) (
              sum by (namespace, pod) (
                  max by (namespace, pod, container) (
                      kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}
                  ) * on(namespace, pod) group_left() max by (namespace, pod) (
                      kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
      - expr: |-
          sum by (namespace) (
              sum by (namespace, pod) (
                  max by (namespace, pod, container) (
                      kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"}
                  ) * on(namespace, pod) group_left() max by (namespace, pod) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                "replicaset", "$1", "owner_name", "(.*)"
              ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                1, max by (replicaset, namespace, owner_name) (
                  kube_replicaset_owner{job="kube-state-metrics"}
                )
              ),
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: deployment
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: daemonset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |-
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: statefulset
        record: namespace_workload_pod:kube_pod_owner:relabel
  kube-apiserver-availability-record-rules:
    groups:
    - interval: 3m
      name: kube-apiserver-availability-record-rules
      rules:
      - expr: |-
          1 - (
            (
              # write too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
              -
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
            ) +
            (
              # read too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"LIST|GET"}[30d]))
              -
              (
                (
                  sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
                  or
                  vector(0)
                )
                +
                sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
                +
                sum(increase(apiserver_request_duration_seconds_bucket{verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
              )
            ) +
            # errors
            sum(code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d)
        labels:
          verb: all
        record: apiserver_request:availability30d
      - expr: |-
          1 - (
            sum(increase(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30d]))
            -
            (
              # too slow
              (
                sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30d]))
                or
                vector(0)
              )
              +
              sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30d]))
              +
              sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30d]))
            )
            +
            # errors
            sum(code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d{verb="read"})
        labels:
          verb: read
        record: apiserver_request:availability30d
      - expr: |-
          1 - (
            (
              # too slow
              sum(increase(apiserver_request_duration_seconds_count{verb=~"POST|PUT|PATCH|DELETE"}[30d]))
              -
              sum(increase(apiserver_request_duration_seconds_bucket{verb=~"POST|PUT|PATCH|DELETE",le="1"}[30d]))
            )
            +
            # errors
            sum(code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
          )
          /
          sum(code:apiserver_request_total:increase30d{verb="write"})
        labels:
          verb: write
        record: apiserver_request:availability30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"2.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"3.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"4.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="LIST",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="GET",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="POST",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PUT",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="PATCH",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code, verb) (increase(apiserver_request_total{job="apiserver",verb="DELETE",code=~"5.."}[30d]))
        record: code_verb:apiserver_request_total:increase30d
      - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
        labels:
          verb: read
        record: code:apiserver_request_total:increase30d
      - expr: sum by (code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
        labels:
          verb: write
        record: code:apiserver_request_total:increase30d
  kube-apiserver-record-rules:
    groups:
    - name: kube-apiserver-record-rules
      rules:
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1d]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1d]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1d]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1d]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
        labels:
          verb: read
        record: apiserver_request:burnrate1d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[1h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[1h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
        labels:
          verb: read
        record: apiserver_request:burnrate1h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[2h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[2h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[2h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[2h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
        labels:
          verb: read
        record: apiserver_request:burnrate2h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30m]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[30m]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[30m]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[30m]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
        labels:
          verb: read
        record: apiserver_request:burnrate30m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[3d]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[3d]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[3d]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[3d]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
        labels:
          verb: read
        record: apiserver_request:burnrate3d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[5m]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[5m]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[5m]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[5m]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: apiserver_request:burnrate5m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[6h]))
              -
              (
                (
                  sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="0.1"}[6h]))
                  or
                  vector(0)
                )
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="0.5"}[6h]))
                +
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[6h]))
              )
            )
            +
            # errors
            sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
        labels:
          verb: read
        record: apiserver_request:burnrate6h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1d]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
        labels:
          verb: write
        record: apiserver_request:burnrate1d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
        labels:
          verb: write
        record: apiserver_request:burnrate1h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[2h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
        labels:
          verb: write
        record: apiserver_request:burnrate2h
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[30m]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
        labels:
          verb: write
        record: apiserver_request:burnrate30m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[3d]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
        labels:
          verb: write
        record: apiserver_request:burnrate3d
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[5m]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: apiserver_request:burnrate5m
      - expr: |-
          (
            (
              # too slow
              sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
              -
              sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[6h]))
            )
            +
            sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
          )
          /
          sum(rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
        labels:
          verb: write
        record: apiserver_request:burnrate6h
      - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
        labels:
          verb: read
        record: code_resource:apiserver_request_total:rate5m
      - expr: sum by (code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
        labels:
          verb: write
        record: code_resource:apiserver_request_total:rate5m
      - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m]))) > 0
        labels:
          quantile: '0.99'
          verb: read
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))) > 0
        labels:
          quantile: '0.99'
          verb: write
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |-
          sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
          /
          sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod)
        record: cluster:apiserver_request_duration_seconds:mean5m
      - expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
  kubelet-record-rules:
    groups:
    - name: kubelet-record-rules
      rules:
      - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.99'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.9'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: '0.5'
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
  kube-prometheus-general-record-rules:
    groups:
    - name: kube-prometheus-general-record-rules
      rules:
      - expr: count without(instance, pod, node) (up == 1)
        record: count:up1
      - expr: count without(instance, pod, node) (up == 0)
        record: count:up0
  kube-prometheus-node-record-rules:
    groups:
    - name: kube-prometheus-node-record-rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m])) BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m])) WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))
        record: cluster:node_cpu:ratio
  kube-scheduler-record-rules:
    groups:
    - name: kube-scheduler-record-rules
      rules:
      - expr: histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.99'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.9'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: '0.5'
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
  node-exporter-record-rules:
    groups:
    - name: node-exporter-record-rules
      rules:
      - expr: |-
          count without (cpu) (
            count without (mode) (
              node_cpu_seconds_total{job="node-exporter"}
            )
          )
        record: instance:node_num_cpu:sum
      - expr: |-
          1 - avg without (cpu, mode) (
            rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
          )
        record: instance:node_cpu_utilisation:rate1m
      - expr: |-
          (
            node_load1{job="node-exporter"}
          /
            instance:node_num_cpu:sum{job="node-exporter"}
          )
        record: instance:node_load1_per_cpu:ratio
      - expr: |-
          1 - (
            node_memory_MemAvailable_bytes{job="node-exporter"}
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        record: instance:node_memory_utilisation:ratio
      - expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
        record: instance:node_vmstat_pgmajfault:rate1m
      - expr: rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_seconds:rate1m
      - expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_bytes_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_bytes_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_drop_excluding_lo:rate1m
      - expr: |-
          sum without (device) (
            rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_drop_excluding_lo:rate1m
  node-record-rules:
    groups:
    - name: node-record-rules
      rules:
      - expr: sum(min(kube_pod_info{node!=""}) by (cluster, node))
        record: ':kube_pod_info_node_count:'
      - expr: |-
          topk by(namespace, pod) (1,
            max by (node, namespace, pod) (
              label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
          ))
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |-
          count by (cluster, node) (sum by (node, cpu) (
            node_cpu_seconds_total{job="node-exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          ))
        record: node:node_num_cpu:sum
      - expr: |-
          sum(
            node_memory_MemAvailable_bytes{job="node-exporter"} or
            (
              node_memory_Buffers_bytes{job="node-exporter"} +
              node_memory_Cached_bytes{job="node-exporter"} +
              node_memory_MemFree_bytes{job="node-exporter"} +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          ) by (cluster)
        record: :node_memory_MemAvailable_bytes:sum
  cronjob-record-rules:
    groups:
    - name: cronjobs-rules
      rules:
        - record: job_cronjob:kube_job_status_start_time:max
          expr: |-
            label_replace(
              label_replace(
                max(
                  kube_job_status_start_time
                  * ON(job_name, namespace) GROUP_RIGHT()
                  kube_job_labels{label_name!=""}
                ) BY (job_name, label_name, namespace)
                == ON(label_name, namespace) GROUP_LEFT()
                max(
                  kube_job_status_start_time
                  * ON(job_name, namespace) GROUP_RIGHT()
                  kube_job_labels{label_name!=""}
                ) BY (label_name, namespace),
                "job", "$1", "job_name", "(.+)"),
              "name", "$1", "label_name", "(.+)")
        - record: job_cronjob:kube_job_status_failed:sum
          expr: |-
            clamp_max(
              job_cronjob:kube_job_status_start_time:max,
            1)
            * ON(job, namespace) GROUP_LEFT()
            label_replace(
              label_replace(
              (kube_job_status_failed != 0),
              "job", "$1", "job_name", "(.+)"),
              "name", "$1", "label_name", "(.+)")
  kubecost:
    groups:
      - name: CPU
        rules:
          - expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m]))
            record: cluster:cpu_usage:rate5m
          - expr: rate(container_cpu_usage_seconds_total{container!=""}[5m])
            record: cluster:cpu_usage_nosum:rate5m
          - expr: avg(irate(container_cpu_usage_seconds_total{container!="POD", container!=""}[5m])) by (container,pod,namespace)
            record: kubecost_container_cpu_usage_irate
          - expr: sum(container_memory_working_set_bytes{container!="POD",container!=""}) by (container,pod,namespace)
            record: kubecost_container_memory_working_set_bytes
          - expr: sum(container_memory_working_set_bytes{container!="POD",container!=""})
            record: kubecost_cluster_memory_working_set_bytes
      - name: Savings
        rules:
          - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod))
            record: kubecost_savings_cpu_allocation
            labels:
              daemonset: "false"
          - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_cpu_allocation) by (pod)) / sum(kube_node_info)
            record: kubecost_savings_cpu_allocation
            labels:
              daemonset: "true"
          - expr: sum(avg(kube_pod_owner{owner_kind!="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod))
            record: kubecost_savings_memory_allocation_bytes
            labels:
              daemonset: "false"
          - expr: sum(avg(kube_pod_owner{owner_kind="DaemonSet"}) by (pod) * sum(container_memory_allocation_bytes) by (pod)) / sum(kube_node_info)
            record: kubecost_savings_memory_allocation_bytes
            labels:
              daemonset: "true"
          - expr: label_replace(sum(kube_pod_status_phase{phase="Running",namespace!="kube-system"} > 0) by (pod, namespace), "pod", "$1", "pod", "(.+)")
            record: kubecost_savings_running_pods
          - expr: sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD",instance!=""}[5m])) by (namespace, pod, container, instance)
            record: kubecost_savings_container_cpu_usage_seconds
          - expr: sum(container_memory_working_set_bytes{container!="",container!="POD",instance!=""}) by (namespace, pod, container, instance)
            record: kubecost_savings_container_memory_usage_bytes
          - expr: avg(sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod, namespace, instance)) by (pod, namespace)
            record: kubecost_savings_pod_requests_cpu_cores
          - expr: avg(sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod, namespace, instance)) by (pod, namespace)
            record: kubecost_savings_pod_requests_memory_bytes
  cost-optimization:
    groups:
      - name: CPU
        rules:
          - record: usage_to_request_ratio
            expr: |-
              sum(
                node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container!="POD"} * on(namespace,pod) group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel) by (pod,container,namespace)
              /sum
              (kube_pod_container_resource_requests{resource="cpu",container!="POD"} * on(namespace,pod) group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel) by (pod,container,namespace) * 100
          - record: usage_to_request_ratio_workload
            expr: |-
              sum(
                node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container!="POD"} * on(namespace,pod) group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel) by (pod,container,namespace,workload)
              /sum
              (kube_pod_container_resource_requests{resource="cpu",container!="POD"} * on(namespace,pod) group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel) by (pod,container,namespace,workload) * 100
  # custom-rules:
  #   groups:
  #   - name: elasticsearch-record-rules
  #     rules:
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d
  #       expr: |
  #         sum(rate(elasticsearch_indices_store_size_bytes_primary{index=~".*logstash-gelf2kafka.*"}[1d]))
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate4h
  #       expr: |
  #         sum(rate(elasticsearch_indices_store_size_bytes_primary{index=~".*logstash-gelf2kafka.*"}[4h]))
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate10h
  #       expr: |
  #         sum(rate(elasticsearch_indices_store_size_bytes_primary{index=~".*logstash-gelf2kafka.*"}[10h]))
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1h
  #       expr: |
  #         sum(rate(elasticsearch_indices_store_size_bytes_primary{index=~".*logstash-gelf2kafka.*"}[1h]))
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d:avg_over_time_1w
  #       expr: |
  #         avg_over_time(elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d[1w])
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d:stddev_over_time_1w
  #       expr: |
  #         stddev_over_time(elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d[1w])
  #     - record: elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:zscore
  #       expr: |
  #         (elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d - elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d:avg_over_time_1w) / elasticsearch_indices_store_size_bytes_primary:logstash_gelf2kafka:rate1d:stddev_over_time_1w
PrometheusAlerts:
  quiet-hours-alert:
    QuietHours:
      enabled: true
      # expr: israel_hour < 9 or israel_hour >= 20
      expr: absent (business_day and business_hour)
      for: 1m
      labels:
        notification: page
        severity: "null"
      annotations:
        description: 'This alert fires during quiet hours. It should be blackholed by Alertmanager.'
  argocd-alerts:
    ArgoCDAppOutOfSync:
      enabled: true
      expr: argocd_app_info{sync_status="OutOfSync", name!="logging-stack"} > 0
      annotations:
        description: "ArgoCD Application {{ $labels.name }} is OutOfSync for longer than 30 minutes"
      for: 30m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppProgressing:
      enabled: true
      expr: argocd_app_info{health_status="Progressing", name!="jaeger"} > 0 #Remove the name!="jaeger" once this solved https://github.com/jaegertracing/helm-charts/issues/207
      annotations:
        description: "ArgoCD Application {{ $labels.name }} Progressing longer than 60 minutes"
      for: 60m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppUnknown:
      enabled: true
      expr: argocd_app_info{sync_status="Unknown"} > 0
      annotations:
        description: "ArgoCD Application {{ $labels.name }} Unknown for 30 minute"
      for: 30m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppMissing:
      enabled: true
      expr: argocd_app_info{health_status="Missing"} > 0
      annotations:
        description: "ArgoCD Application {{ $labels.name }} Missing for 30 minutes"
      for: 30m
      labels:
        severity: warning
        controlPlane: true
    ArgoCDAppDegraded:
      enabled: true
      expr: argocd_app_info{health_status="Degraded"} > 0
      annotations:
        description: "ArgoCD Application {{ $labels.name }} Degraded for 30 minutes"
      for: 30m
      labels:
        severity: warning
        controlPlane: true
  istio-alerts:
    IstioPilotAvailabilityDrop:
      annotations:
        summary: 'Istio Pilot Availability Drop'
        description: 'Pilot pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
      enabled: true
      expr: >
        avg(avg_over_time(up{job="pilot"}[1m])) < 1
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioPilotPushErrorsHigh:
      annotations:
        summary: 'Number of Istio Pilot push errors is too high'
        description: 'Pilot has too many push errors during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
      enabled: true
      expr: >
        sum(irate(pilot_xds_push_errors{job="pilot"}[5m])) / sum(irate(pilot_xds_pushes{job="pilot"}[5m])) > 0.05
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGatewayAvailabilityDrop:
      annotations:
        summary: 'Istio Gateway Availability Drop'
        description: 'Gateway pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic will likely be affected'
      enabled: true
      expr: >
        min(kube_deployment_status_replicas_available{namespace="istio-ingress"}) without (instance, pod) < 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    # IstioIngressRequestRateHigh:
    #   annotations:
    #     summary: 'Istio Ingress Gateway Request Rate High'
    #     description: 'Istio ingress gateway request rate is unusually high during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The amount of traffic being generated from outside the cluster is higher than normal'
    #   enabled: true
    #   expr: >
    #     round(sum(irate(istio_requests_total{reporter="source",source_app!="infra-lb"}[5m])), 0.001) > 1200
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    IstioIngressHTTP5xxRateHigh:
      annotations:
        summary: 'Istio Percentage of HTTP 5xx responses replyed by the ingress gateways is too high'
        description: 'Istio HTTP 5xx rate is too high in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The HTTP 5xx errors replyed by the ingress gateways is unusually high'
      enabled: true
      expr: >
        sum(irate(istio_requests_total{reporter="source",source_app!="infra-lb", response_code=~"5.*"}[5m])) / sum(irate(istio_requests_total{reporter="source",source_app!="infra-lb"}[5m])) > 10
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    # IstioGlobalRequestRateHigh:
    #   annotations:
    #     summary: 'Istio Global Request Rate High'
    #     description: 'Istio global request rate is unusually high during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The amount of traffic being generated inside the service mesh is higher than normal'
    #   enabled: true
    #   expr: >
    #     round(sum(irate(istio_requests_total{reporter="source"}[5m])), 0.001) > 1200
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    # IstioGlobalRequestRateLow:
    #   annotations:
    #     summary: 'Istio global request rate too low'
    #     description: 'Istio global request rate is unusually low during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The amount of traffic being generated inside the service mesh has dropped below usual levels'
    #   enabled: true
    #   expr: >
    #     round(sum(irate(istio_requests_total{reporter="source"}[5m])), 0.001) < 300
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    IstioGlobalHTTP5xxRateHigh:
      annotations:
        summary: 'Istio Percentage of HTTP 5xx responses is too high'
        description: 'Istio global HTTP 5xx rate is too high in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The HTTP 5xx errors within the service mesh is unusually high'
      enabled: true
      expr: >
        sum(irate(istio_requests_total{reporter="source", response_code=~"5.*"}[5m])) / sum(irate(istio_requests_total{reporter="destination"}[5m])) > 0.01
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    IstioGatewayOutgoingSuccessLow:
      annotations:
        summary: 'Istio Gateway outgoing success rate is too low'
        description: 'Istio Gateway success to outbound destinations is too low in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic may be affected'
      enabled: true
      expr: >
        sum(irate(istio_requests_total{reporter="source", source_workload="istio-ingressgateway",source_workload_namespace="istio-system", connection_security_policy!="mutual_tls",response_code!~"5.*"}[5m])) /  sum(irate(istio_requests_total{reporter="source", source_workload="istio-ingressgateway",source_workload_namespace="istio-system", connection_security_policy!="mutual_tls"}[5m])) < 0.995
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    # IstioPilotNoIp:
    #   annotations:
    #     summary: 'Pods not found in the endpoint table, possibly invalid.'
    #     description: 'Pods not found in the endpoint table, possibly invalid.'
    #   enabled: true
    #   expr: pilot_no_ip > 0
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    # IstioClustersWithoutInstances:
    #   annotations:
    #     summary: 'Number of clusters without instances.'
    #     description: 'Number of clusters without instances.'
    #   enabled: true
    #   expr: pilot_eds_no_instances > 0
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    # IstioConflictingInboundListeners:
    #   annotations:
    #     summary: 'Number of conflicting inbound listeners.'
    #     description: 'Number of conflicting inbound listeners.'
    #   enabled: true
    #   expr: pilot_eds_no_instances > 0
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
  prometheus-alerts:
    # PrometheusTargetMissing:
    #   enabled: true
    #   expr: up == 0
    #   for: 0m
    #   labels:
    #     severity: critical
    #     controlPlane: true
    #   annotations:
    #     summary: "Prometheus target missing (instance {{ $labels.instance }})"
    #     description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAllTargetsMissing:
      enabled: true
      expr: count by (job) (up) == 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus all targets missing (instance {{ $labels.instance }})"
        description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusConfigurationReloadFailure:
      enabled: true
      expr: prometheus_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
        description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTooManyRestarts:
      enabled: true
      expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
        description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerConfigurationReloadFailure:
      enabled: true
      expr: alertmanager_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
        description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerConfigNotSynced:
      enabled: true
      expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager config not synced (instance {{ $labels.instance }})"
        description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusNotConnectedToAlertmanager:
      enabled: true
      expr: prometheus_notifications_alertmanagers_discovered < 1
      for: 1m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
        description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # PrometheusRuleEvaluationFailures:
    #   enabled: true
    #   expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
    #   for: 0m
    #   labels:
    #     severity: critical
    #   annotations:
    #     summary: "Prometheus rule evaluation failures (instance {{ $labels.instance }})"
    #     description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTemplateTextExpansionFailures:
      enabled: true
      expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus template text expansion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusRuleEvaluationSlow:
      enabled: true
      expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus rule evaluation slow (instance {{ $labels.instance }})"
        description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusNotificationsBacklog:
      enabled: true
      expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus notifications backlog (instance {{ $labels.instance }})"
        description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusAlertmanagerNotificationFailing:
      enabled: true
      expr: rate(alertmanager_notifications_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
        description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetEmpty:
      enabled: true
      expr: |
        prometheus_sd_discovered_targets{
        config!="kubecost",
        config!="serviceMonitor/monitoring/telepresence/0"} == 0
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus target empty (instance {{ $labels.instance }})"
        description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetScrapingSlow:
      enabled: true
      expr: prometheus_target_interval_length_seconds{quantile="0.9", interval!="10m0s"} > 60
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus target scraping slow (instance {{ $labels.instance }})"
        description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusLargeScrape:
      enabled: true
      expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus large scrape (instance {{ $labels.instance }})"
        description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTargetScrapeDuplicate:
      enabled: true
      expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Prometheus target scrape duplicate (instance {{ $labels.instance }})"
        description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCheckpointCreationFailures:
      enabled: true
      expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCheckpointDeletionFailures:
      enabled: true
      expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbCompactionsFailed:
      enabled: true
      expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbHeadTruncationsFailed:
      enabled: true
      expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbReloadFailures:
      enabled: true
      expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbWalCorruptions:
      enabled: true
      expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusTsdbWalTruncationsFailed:
      enabled: true
      expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    PrometheusBadConfig:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to reload its configuration.
        summary: Failed Prometheus configuration reload.
      enabled: true
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful[5m]) == 0
      for: 10m
      labels:
        severity: critical
        controlPlane: true
    PrometheusNotificationQueueRunningFull:
      annotations:
        description: Alert notification queue of Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is running full.
        summary: Prometheus alert notification queue predicted to run full in less than 30m.
      enabled: true
      expr: |-
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity[5m])
        )
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusErrorSendingAlertsToSomeAlertmanagers:
      annotations:
        description: '{{  printf "%.1f" $value  }}% errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager {{ $labels.alertmanager }}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      enabled: true
      expr: |-
        (
          rate(prometheus_notifications_errors_total[5m])
        /
          rate(prometheus_notifications_sent_total[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusErrorSendingAlertsToAnyAlertmanager:
      annotations:
        description: '{{  printf "%.1f" $value  }}% minimum errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to any Alertmanager.'
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      enabled: true
      expr: |-
        min without(alertmanager) (
          rate(prometheus_notifications_errors_total[5m])
        /
          rate(prometheus_notifications_sent_total[5m])
        )
        * 100
        > 3
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusNotConnectedToAlertmanagers:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      enabled: true
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered[5m]) < 1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusTSDBReloadsFailing:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      enabled: true
      expr: increase(prometheus_tsdb_reloads_failures_total[3h]) > 0
      for: 4h
      labels:
        severity: warning
        controlPlane: true
    PrometheusTSDBCompactionsFailing:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      enabled: true
      expr: increase(prometheus_tsdb_compactions_failed_total[3h]) > 0
      for: 4h
      labels:
        severity: warning
        controlPlane: true
    PrometheusNotIngestingSamples:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not ingesting samples.
        summary: Prometheus is not ingesting samples.
      enabled: true
      expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusDuplicateTimestamps:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{  printf "%.4g" $value   }} samples/s with different values but duplicated timestamp.
        summary: Prometheus is dropping samples with duplicate timestamps.
      enabled: true
      expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOutOfOrderTimestamps:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{  printf "%.4g" $value   }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      enabled: true
      expr: rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusRemoteStorageFailures:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} failed to send {{  printf "%.1f" $value  }}% of the samples to {{  $labels.remote_name }}:{{  $labels.url  }}
        summary: Prometheus fails to send samples to remote storage.
      enabled: true
      expr: |-
        (
          rate(prometheus_remote_storage_failed_samples_total[5m])
        /
          (
            rate(prometheus_remote_storage_failed_samples_total[5m])
          +
            rate(prometheus_remote_storage_succeeded_samples_total[5m])
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusRemoteWriteBehind:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write is {{  printf "%.1f" $value  }}s behind for {{  $labels.remote_name }}:{{  $labels.url  }}.
        summary: Prometheus remote write is behind.
      enabled: true
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds[5m])
        - on(job, instance) group_right
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds[5m])
        )
        > 120
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusRemoteWriteDesiredShards:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write desired shards calculation wants to run {{  $value  }} shards for queue {{  $labels.remote_name }}:{{  $labels.url  }}, which is more than the max of {{  printf `prometheus_remote_storage_shards_max{instance="%s"}` $labels.instance | query | first | value  }}.
        summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
      enabled: true
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max[5m])
        )
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusRuleFailures:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to evaluate {{  printf "%.0f" $value  }} rules in the last 5m.
        summary: Prometheus is failing rule evaluations.
      enabled: true
      expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    PrometheusMissingRuleEvaluations:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has missed {{  printf "%.0f" $value  }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      enabled: true
      expr: increase(prometheus_rule_group_iterations_missed_total[5m]) > 0
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusTargetLimitHit:
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has dropped {{  printf "%.0f" $value  }} targets because the number of targets exceeded the configured target_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
      enabled: true
      expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total[5m]) > 0
      for: 15m
      labels:
        severity: warning
        controlPlane: true
  prometheus-operator-alerts:
    PrometheusOperatorListErrors:
      annotations:
        description: Errors while performing List operations in controller {{ $labels.controller }} in {{ $labels.namespace }} namespace.
        summary: Errors while performing list operations in controller.
      enabled: true
      expr: (sum by (controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m])) / sum by (controller,namespace) (rate(prometheus_operator_list_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorWatchErrors:
      annotations:
        description: Errors while performing watch operations in controller {{ $labels.controller }} in {{ $labels.namespace }} namespace.
        summary: Errors while performing watch operations in controller.
      enabled: true
      expr: (sum by (controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m])) / sum by (controller,namespace) (rate(prometheus_operator_watch_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorSyncFailed:
      annotations:
        description: Controller {{  $labels.controller  }} in {{  $labels.namespace  }} namespace fails to reconcile {{  $value  }} objects.
        summary: Last controller reconciliation failed
      enabled: true
      expr: min_over_time(prometheus_operator_syncs{status="failed",job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorReconcileErrors:
      annotations:
        description: '{{  $value | humanizePercentage  }} of reconciling operations failed for {{  $labels.controller  }} controller in {{  $labels.namespace  }} namespace.'
        summary: Errors while reconciling controller.
      enabled: true
      expr: (sum by (controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]))) / (sum by (controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorNodeLookupErrors:
      annotations:
        description: Errors while reconciling Prometheus in {{  $labels.namespace  }} Namespace.
        summary: Errors while reconciling Prometheus.
      enabled: true
      expr: rate(prometheus_operator_node_address_lookup_errors_total{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorNotReady:
      annotations:
        description: Prometheus operator in {{  $labels.namespace  }} namespace isn't ready to reconcile {{  $labels.controller  }} resources.
        summary: Prometheus operator not ready
      enabled: true
      expr: min by(namespace, controller) (max_over_time(prometheus_operator_ready{job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) == 0)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    PrometheusOperatorRejectedResources:
      annotations:
        description: Prometheus operator in {{  $labels.namespace  }} namespace rejected {{  printf "%0.0f" $value  }} {{  $labels.controller  }}/{{  $labels.resource  }} resources.
        summary: Resources rejected by Prometheus operator
      enabled: true
      expr: min_over_time(prometheus_operator_managed_resources{state="rejected",job="{{ $operatorJob }}",namespace="{{ $namespace }}"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
        controlPlane: true
  k8s-node-alerts:
    HostOutOfMemory:
      enabled: true
      expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of memory (instance {{ $labels.instance }})"
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostMemoryUnderMemoryPressure:
      enabled: true
      expr: rate(node_vmstat_pgmajfault[1m]) > 1000
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
        description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostUnusualNetworkThroughputIn:
    #   enabled: true
    #   expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 300
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
    #     description: "Host network interfaces are probably receiving too much data (> 300 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostUnusualNetworkThroughputOut:
    #   enabled: true
    #   expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 300
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
    #     description: "Host network interfaces are probably sending too much data (> 300 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostUnusualDiskReadRate:
    #   enabled: true
    #   expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 100
    #   for: 5m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host unusual disk read rate (instance {{ $labels.instance }})"
    #     description: "Disk is probably reading too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostUnusualDiskWriteRate:
    #   enabled: true
    #   expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 500
    #   for: 2m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host unusual disk write rate (instance {{ $labels.instance }})"
    #     description: "Disk is probably writing too much data (> 500 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # Please add ignored mountpoints in node_exporter parameters like
    # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
    # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
    HostOutOfDiskSpace:
      enabled: true
      expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of disk space (instance {{ $labels.instance }})"
        description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # Please add ignored mountpoints in node_exporter parameters like
    # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
    # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
    HostDiskWillFillIn24Hours:
      enabled: true
      expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host disk will fill in 24 hours (instance {{ $labels.instance }})"
        description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostOutOfInodes:
      enabled: true
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host out of inodes (instance {{ $labels.instance }})"
        description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostInodesWillFillIn24Hours:
      enabled: true
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host inodes will fill in 24 hours (instance {{ $labels.instance }})"
        description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskReadLatency:
      enabled: true
      expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
      for: 30m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk read latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostUnusualDiskWriteLatency:
      enabled: true
      expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0
      for: 30m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host unusual disk write latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostHighCpuLoadWarning:
      enabled: true
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 95
      for: 30m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host high CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 95%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostHighCpuLoadCritical:
      enabled: true
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 95
      for: 60m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host high CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 95%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostCpuStealNoisyNeighbor:
      enabled: true
      expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host CPU steal noisy neighbor (instance {{ $labels.instance }})"
        description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # 1000 context switches is an arbitrary number.
    # Alert threshold depends on nature of application.
    # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
    # HostContextSwitching:
    #   enabled: true
    #   expr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode="idle"})) > 20000
    #   for: 0m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host context switching (instance {{ $labels.instance }})"
    #     description: "Context switching is growing on node (> 20000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostSwapIsFillingUp:
      enabled: true
      expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host swap is filling up (instance {{ $labels.instance }})"
        description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostSystemdServiceCrashed:
      enabled: true
      expr: node_systemd_unit_state{state="failed"} == 1
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host SystemD service crashed (instance {{ $labels.instance }})"
        description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostPhysicalComponentTooHot:
      enabled: true
      expr: node_hwmon_temp_celsius > 75
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host physical component too hot (instance {{ $labels.instance }})"
        description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNodeOvertemperatureAlarm:
      enabled: true
      expr: node_hwmon_temp_crit_alarm_celsius == 1
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host node overtemperature alarm (instance {{ $labels.instance }})"
        description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostRaidArrayGotInactive:
      enabled: true
      expr: node_md_state{state="inactive"} > 0
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Host RAID array got inactive (instance {{ $labels.instance }})"
        description: "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostRaidDiskFailure:
      enabled: true
      expr: node_md_disks{state="failed"} > 0
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host RAID disk failure (instance {{ $labels.instance }})"
        description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostKernelVersionDeviations:
    #   enabled: true
    #   expr: count(sum(label_replace(node_uname_info, "kernel", "$1", "release", "([0-9]+.[0-9]+.[0-9]+).*")) by (kernel)) > 1
    #   for: 6h
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host kernel version deviations (instance {{ $labels.instance }})"
    #     description: "Different kernel versions are running\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # HostOomKillDetected:
    #   enabled: true
    #   expr: increase(node_vmstat_oom_kill[1m]) > 0
    #   for: 0m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     summary: "Host OOM kill detected (instance {{ $labels.instance }})"
    #     description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostEdacCorrectableErrorsDetected:
      enabled: true
      expr: increase(node_edac_correctable_errors_total[1m]) > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host EDAC Correctable Errors detected (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} has had {{ printf \"%.0f\" $value }} correctable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostEdacUncorrectableErrorsDetected:
      enabled: true
      expr: node_edac_uncorrectable_errors_total > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} has had {{ printf \"%.0f\" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkReceiveErrors:
      enabled: true
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Receive Errors (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkTransmitErrors:
      enabled: true
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Transmit Errors (instance {{ $labels.instance }})"
        description: "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostNetworkInterfaceSaturated:
      enabled: true
      expr: (rate(node_network_receive_bytes_total{device!~"^tap.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*"} > 0.8
      for: 15m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host Network Interface Saturated (instance {{ $labels.instance }})"
        description: "The network interface {{ $labels.interface }} on {{ $labels.instance }} is getting overloaded.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostConntrackLimit:
      enabled: true
      expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host conntrack limit (instance {{ $labels.instance }})"
        description: "The number of conntrack is approching limit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostClockSkew:
      enabled: true
      expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host clock skew (instance {{ $labels.instance }})"
        description: "Clock skew detected. Clock is out of sync.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    HostClockNotSynchronising:
      enabled: true
      expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Host clock not synchronising (instance {{ $labels.instance }})"
        description: "Clock not synchronising.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    K8SNodePodNumberUtilization:
      enabled: true
      expr: |
          (node:node_running_pod_count:sum / on(kubernetes_cluster, instance_name) node:node_status_pods_capacity: * 100) > 95
      for: 30m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Nodes number of POD ultilization has been over 95% for more than 5m."
        summary: "Too many pods"
    K8SNodeCPUSaturation:
      enabled: true
      expr: |
        (sum by (kubernetes_cluster, instance_name)(node_load1{job=~"kubernetes_.+"}) / node:cpu:sum * 100) > 120
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node CPU load has been saturated over 100% for more than 5m."
        summary: "CPU requests cannot be fulfilled because of unavailability.  "
    K8SNodeCPUUtilization:
      enabled: true
      expr: |
        (node:node_container_cpu_usage_seconds:irate1m /  node:node_allocatable_cpu_cores:sum) * 100 > 90
      for: 5m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node CPU ultilization by containers has been over 90% for more than 5m."
        summary: "Node CPU ultilization by containers is high. Recalculate CPU request and limits."
    K8SNodeMemoryUtilization:
      enabled: true
      expr: |
        (node:node_container_memory_wss_bytes:sum / ON (kubernetes_cluster, instance_name) node:node_allocatable_memory_bytes:sum * 100) > 90
      for: 5m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node memory ultilization by containers has been over 90% for more than 5m."
        summary: "Node memory ultilization by containers is high. Recalculate Memory request and limits."
    K8SNodeNetworkErrors:
      enabled: true
      expr: |
        ((rate(node_network_transmit_errs_total{job=~"kubernetes_.+", device!~"veth.+"}[1m]) == 0) + (rate(node_network_receive_errs_total{job=~"kubernetes_.+", device!~"veth.+"}[1m]) == 0)) > 0
      for: 1m
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Node network interface {{ $labels.device }} showing errors."
        summary: "Network errors deteced on interface."
    K8SNodeNetworkInterfaceFlapping:
      enabled: true
      expr: changes(node_network_up{job="kubernetes_.+", interface!~"veth.+"}[1m]) > 2
      for: 1m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node network interface {{ $labels.interface }} flapping."
        summary: "Network interface flapping detected."
    K8SNodeFileSystemRunningFull24H:
      enabled: true
      expr: (node:node_filesystem_usage:ratio > 0.85) and (predict_linear(node:node_filesystem_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Node filesystem will be full within the next 24 hours."
        summary: "Filesystem on node running out of disk."
    K8SNodeFileSystemRunningFull2H:
      enabled: true
      expr: (node:node_filesystem_usage:ratio > 0.85) and (predict_linear(node:node_filesystem_avail:ratio[6h], 3600 * 2) < 0)
      for: 30m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Node filesystem will be full within the next 2 hours."
        summary: "Filesystem on node running out of disk."
  kube-state-metrics-alerts:
    KubernetesNodeReady:
      enabled: true
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 2h
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
        description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesMemoryPressure:
      enabled: true
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesDiskPressure:
      enabled: true
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesOutOfDisk:
      enabled: true
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesOutOfCapacity:
      enabled: true
      expr: sum(kube_pod_info) by (node) / sum(kube_node_status_allocatable_pods) by (node) * 100 > 90
      for: 2m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes out of capacity (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} is out of capacity\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    ConfigmapAttacherJobFailed:
      enabled: true
      expr: kube_job_status_failed{namespace="kube-system", job_name=~"configmap-attacher-job.*|cm-att.*"} > 0
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes configmap-attacher-job Job failed (instance {{ $labels.instance }})"
        description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesApiServerErrors:
      enabled: true
      expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_count{job="apiserver"}[1m])) * 100 > 3
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
        description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesApiClientErrors:
      enabled: true
      expr: (sum(rate(rest_client_requests_total{code=~"5.."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
        description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesClientCertificateExpiresNextWeek:
      enabled: true
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
      for: 0m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        summary: "Kubernetes client certificate expires next week (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesClientCertificateExpiresSoon:
      enabled: true
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
      for: 0m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        summary: "Kubernetes client certificate expires soon (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  cluster-autoscaler-alerts:
    # ClusterAutoScalerScaling30m:
    #   enabled: true
    #   expr: abs(sum(cluster_autoscaler_scaled_up_nodes_total)-sum(cluster_autoscaler_scaled_down_nodes_total)) >=1
    #   for: 30m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     description: Too many nodes scaling in the last 30m {{ $value }} node(s)
    #     summary: Kube Cluster Autoscaler is scaling down or up in the last 30 minutes
    # ClusterAutoScalerScaling60m:
    #   enabled: true
    #   expr: abs(sum(cluster_autoscaler_scaled_up_nodes_total)-sum(cluster_autoscaler_scaled_down_nodes_total)) >=1
    #   for: 60m
    #   labels:
    #     severity: warning
    #     controlPlane: true
    #   annotations:
    #     description: Too many nodes scaling in the last 60m {{ $value }} node(s)
    #     summary: Kube Cluster Autoscaler is scaling down or up in the last 60 minutes
    ClusterAutoScalerUnschedulablePods10m:
      enabled: true
      expr: sum(cluster_autoscaler_unschedulable_pods_count) > 1
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: Pods can't be scheduled for 10 minutes {{ $value }}
        summary: Unscheduled pods for 10 minutes
    ClusterAutoScalerUnschedulablePods120m:
      enabled: true
      expr: sum(cluster_autoscaler_unschedulable_pods_count) > 1
      for: 2h
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Pods can't be scheduled for 1 hour {{ $value }}
        summary: Unscheduled pods for 1 hour
    ClusterAutoScalerErrors:
      enabled: true
      expr: increase(cluster_autoscaler_errors_total[5m]) > 0
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Cluster Autoscaler has errors {{ $value }}
        summary: Cluster Autoscaler has errors
  core-dns-alerts:
    K8SCoreDNSErrorRateZScore:
      enabled: true
      expr: 'coredns:error_rate_zscore_abs > 5'
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Error rate is more than 5 for 30 min"
    K8SCoreDNSRequestsZScore:
      enabled: true
      expr: 'coredns:requests_rate_zscore_abs > 5'
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Request rate is more than 5 for 30 min"
    K8SCoreDNSCacheHitZScore:
      enabled: true
      expr: 'coredns:cache_hits_rate_zscore_abs > 5'
      for: 30m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: "ZScore for CoreDNS Cache Hit rate is more than 5 for 30 min"
  k8s-cluster-alerts:
    K8SStateMetricsDown:
      enabled: true
      expr: absent(up{container=~".*state-metrics.*"} == 1)
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "KubeStateMetrics has disappeared from Prometheus target discovery for longer than 10m."
        summary: "KubeStateMetrics is probably down."
    K8SVersionMismatch:
      enabled: true
      expr: count by(kubernetes_cluster)(count by (kubernetes_cluster, gitVersion) (label_replace(kubernetes_build_info{job=~"kubernetes_node.+"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "There has been different semantic versions of Kubernetes components running for more than 1h."
        summary: "NodeExporter is probably down."
    K8SClusterCPUOvercommit:
      enabled: true
      expr: sum by(kubernetes_cluster) (namespace:kube_pod_container_resource_requests_cpu_cores:sum) / sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores{job=~"kubernetes_.+"}) - (sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores) - 1) / sum by(kubernetes_cluster) (kube_node_status_allocatable_cpu_cores) > 0
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure for more than 1h."
        summary: ""
    K8SClusterMemoryOvercommit:
      enabled: true
      expr: |
        sum by (kubernetes_cluster)(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes{job=~"kubernetes_.+"})
            -
          (sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes)-1) / sum by (kubernetes_cluster)(kube_node_status_allocatable_memory_bytes) > 0
      for: 1h
      labels:
        controlPlane: true
        severity: warning
      annotations:
        description: "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure for more than 1h."
        summary: ""
    K8SClusterClientCertificateExpiration:
      enabled: true
      expr: |
        apiserver_client_certificate_expiration_seconds_count{job=~"kubernetes_.+"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=~"kubernetes_.+"}[5m]))) < 86400
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Kubernetes client certificate used to authenticate to the apiserver is expiring in less than 2 hours"
        summary: ""
  k8s-namespace-alerts:
    K8SNamespaceCPUOvercommit:
      enabled: true
      expr: |
        (sum by(kubernetes_cluster, namespace, resourcequota)(kube_resourcequota{job=~"kubernetes_.+", type="hard", resource="cpu"})
        / on (kubernetes_cluster) group_left()
        sum by(kubernetes_cluster)(kube_node_status_allocatable_cpu_cores)) * 100 > 100
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has overcommitted CPU resource requests for longer than 10m."
    K8SNamespaceMemoryOvercommit:
      enabled: true
      expr: |
          (sum by(kubernetes_cluster, namespace, resourcequota)(kube_resourcequota{job=~"kubernetes_.+", type="hard", resource="memory"})
            / on (kubernetes_cluster) group_left()
          sum by(kubernetes_cluster)(kube_node_status_allocatable_memory_bytes)) * 100
            > 100
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has overcommitted memory resource requests for longer than 10m."
    K8SNamespaceQuotaExceeded:
      enabled: true
      expr: |
          (kube_resourcequota{job=~"kubernetes_.+", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{job=~"kubernetes_.+", type="hard"} > 0)) * 100
            > 90
      for: 10m
      labels:
        controlPlane: true
        severity: critical
      annotations:
        description: "Namespace has been using more resource than 90% of quota for longer than 10m."
  alertmanager-alerts:
    AlertmanagerConfigInconsistent:
      annotations:
        description: 'The configuration of the instances of the Alertmanager cluster `{{  $labels.namespace  }}/{{  $labels.service  }}` are out of sync.

          {{  range printf "alertmanager_config_hash{namespace=\"%s\",service=\"%s\"}" $labels.namespace $labels.service | query  }}

          Configuration hash for pod {{  .Labels.pod  }} is "{{  printf "%.f" .Value  }}"

          {{  end  }}

          '
      enabled: true
      expr: count by(namespace,service) (count_values by(namespace,service) ("config_hash", alertmanager_config_hash{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"})) != 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
    AlertmanagerFailedReload:
      annotations:
        description: Reloading Alertmanager's configuration has failed for {{  $labels.namespace  }}/{{  $labels.pod }}.
      enabled: true
      expr: alertmanager_config_last_reload_successful{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"} == 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    AlertmanagerMembersInconsistent:
      annotations:
        description: Alertmanager has not found all other members of the cluster.
      enabled: true
      expr: |-
        alertmanager_cluster_members{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"}
          != on (service) GROUP_LEFT()
        count by (service) (alertmanager_cluster_members{job="{{ $alertmanagerJob }}",namespace="{{ $namespace }}"})
      for: 5m
      labels:
        severity: critical
        controlPlane: true
  kube-apiserver-slos-alerts:
    KubeAPIErrorBudgetBurn2M:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      enabled: true
      expr: |-
        sum(apiserver_request:burnrate1h) > (14.40 * 0.01000)
        and
        sum(apiserver_request:burnrate5m) > (14.40 * 0.01000)
      for: 2m
      labels:
        severity: critical
        controlPlane: true
    KubeAPIErrorBudgetBurn15M:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      enabled: true
      expr: |-
        sum(apiserver_request:burnrate6h) > (6.00 * 0.01000)
        and
        sum(apiserver_request:burnrate30m) > (6.00 * 0.01000)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    KubeAPIErrorBudgetBurn1H:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      enabled: true
      expr: |-
        sum(apiserver_request:burnrate1d) > (3.00 * 0.01000)
        and
        sum(apiserver_request:burnrate2h) > (3.00 * 0.01000)
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    KubeAPIErrorBudgetBurn3H:
      annotations:
        description: The API server is burning too much error budget.
        summary: The API server is burning too much error budget.
      enabled: true
      expr: |-
        sum(apiserver_request:burnrate3d) > (1.00 * 0.01000)
        and
        sum(apiserver_request:burnrate6h) > (1.00 * 0.01000)
      for: 3h
      labels:
        severity: warning
        controlPlane: true
  kubernetes-resources-alerts:
    KubeCPUOvercommit:
      annotations:
        description: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.
        summary: Cluster has overcommitted CPU resource requests.
      enabled: true
      expr: |-
        sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          >
        (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeMemoryOvercommit:
      annotations:
        description: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.
        summary: Cluster has overcommitted memory resource requests.
      enabled: true
      expr: |-
        sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
          /
        sum(kube_node_status_allocatable_memory_bytes)
          >
        (count(kube_node_status_allocatable_memory_bytes)-1)
          /
        count(kube_node_status_allocatable_memory_bytes)
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeCPUQuotaOvercommit:
      annotations:
        description: Cluster has overcommitted CPU resource requests for Namespaces.
        summary: Cluster has overcommitted CPU resource requests.
      enabled: true
      expr: |-
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          > 1.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeMemoryQuotaOvercommit:
      annotations:
        description: Cluster has overcommitted memory resource requests for Namespaces.
        summary: Cluster has overcommitted memory resource requests.
      enabled: true
      expr: |-
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
          /
        sum(kube_node_status_allocatable_memory_bytes{job="kube-state-metrics"})
          > 1.5
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaAlmostFull:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota is going to be full.
      enabled: true
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 0.9 < 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaFullyUsed:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota is fully used.
      enabled: true
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          == 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeQuotaExceeded:
      annotations:
        description: Namespace {{  $labels.namespace  }} is using {{  $value | humanizePercentage  }} of its {{  $labels.resource  }} quota.
        summary: Namespace quota has exceeded the limits.
      enabled: true
      expr: |-
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
  kubernetes-system-apiserver-alerts:
    AggregatedAPIErrors:
      annotations:
        description: Kubernetes aggregated API {{  $labels.name  }}/{{  $labels.namespace  }} has reported errors. It has appeared unavailable {{  $value | humanize  }}% times averaged over the past 10m.
        summary: An aggregated API has reported errors.
      enabled: true
      expr: sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total[10m])) > 4
      labels:
        severity: warning
        controlPlane: true
    AggregatedAPIDown:
      annotations:
        description: An aggregated API {{  $labels.name  }}/{{  $labels.namespace  }} has been only {{  $value | humanize  }}% available over the last 30m.
        summary: An aggregated API is down.
      enabled: true
      expr: (1 - max by(name, namespace)(avg_over_time(aggregator_unavailable_apiservice[30m]))) * 100 < 80
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeAPIDown:
      annotations:
        description: KubeAPI has disappeared from Prometheus target discovery.
        summary: Target disappeared from Prometheus target discovery.
      enabled: true
      expr: absent(up{job="apiserver"} == 1)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
  kubernetes-system-kubelet-alerts:
    KubeNodeUnreachable:
      annotations:
        description: '{{  $labels.node  }} is unreachable and some workloads may be rescheduled.'
        summary: Node is unreachable.
      enabled: true
      expr: (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletTooManyPods:
      annotations:
        description: Kubelet '{{  $labels.node  }}' is running at {{  $value | humanizePercentage  }} of its Pod capacity.
        summary: Kubelet is running at capacity.
      enabled: true
      expr: |-
        count by(node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        )
        /
        max by(node) (
          kube_node_status_capacity_pods{job="kube-state-metrics"} != 1
        ) > 0.95
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeNodeReadinessFlapping:
      annotations:
        description: The readiness status of node {{  $labels.node  }} has changed {{  $value  }} times in the last 15 minutes.
        summary: Node readiness status is flapping.
      enabled: true
      expr: sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (node) > 2
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletPlegDurationHigh:
      annotations:
        description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{  $value  }} seconds on node {{  $labels.node  }}.
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      enabled: true
      expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    KubeletPodStartUpLatencyHigh:
      annotations:
        description: Kubelet Pod startup 99th percentile latency is {{  $value  }} seconds on node {{  $labels.node  }}.
        summary: Kubelet Pod startup latency is too high.
      enabled: true
      expr: histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeletDown:
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        summary: Target disappeared from Prometheus target discovery.
      enabled: true
      expr: absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    # KubeletRuntimeErrors:
    #   annotations:
    #     description: Kubelet has errors.
    #     summary: Kubelet has errors.
    #   enabled: true
    #   expr: increase(kubelet_runtime_operations_errors_total[1m]) > 0
    #   for: 5m
    #   labels:
    #     severity: critical
    #     controlPlane: true
  kubernetes-system-alerts:
    KubeVersionMismatch:
      annotations:
        description: There are {{  $value  }} different semantic versions of Kubernetes components running.
        summary: Different semantic versions of Kubernetes components running.
      enabled: true
      expr: count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*).*"))) > 1
      for: 15m
      labels:
        severity: warning
        controlPlane: true
    KubeClientErrors:
      annotations:
        description: Kubernetes API server client '{{  $labels.job  }}/{{  $labels.instance  }}' is experiencing {{  $value | humanizePercentage  }} errors.'
        summary: Kubernetes API server client is experiencing errors.
      enabled: true
      expr: |-
        (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
          /
        sum(rate(rest_client_requests_total[5m])) by (instance, job))
        > 0.01
      for: 15m
      labels:
        severity: warning
        controlPlane: true
  node-exporter-alerts:
    NodeFilesystemSpaceFillingUp24H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left and is filling up.
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      enabled: false
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemSpaceFillingUp4H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left and is filling up fast.
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      enabled: true
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemAlmostOutOfSpace5%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left.
        summary: Filesystem has less than 5% space left.
      enabled: true
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemAlmostOutOfSpace3%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available space left.
        summary: Filesystem has less than 3% space left.
      enabled: true
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemFilesFillingUp24H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left and is filling up.
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      enabled: true
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemFilesFillingUp4H:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left and is filling up fast.
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      enabled: true
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeFilesystemAlmostOutOfFiles5%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left.
        summary: Filesystem has less than 5% inodes left.
      enabled: true
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeFilesystemAlmostOutOfFiles3%:
      annotations:
        description: Filesystem on {{  $labels.device  }} at {{  $labels.instance  }} has only {{  printf "%.2f" $value  }}% available inodes left.
        summary: Filesystem has less than 3% inodes left.
      enabled: true
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    NodeNetworkReceiveErrs:
      annotations:
        description: '{{  $labels.instance  }} interface {{  $labels.device  }} has encountered {{  printf "%.0f" $value  }} receive errors in the last two minutes.'
        summary: Network interface is reporting many receive errors.
      enabled: true
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    NodeNetworkTransmitErrs:
      annotations:
        description: '{{  $labels.instance  }} interface {{  $labels.device  }} has encountered {{  printf "%.0f" $value  }} transmit errors in the last two minutes.'
        summary: Network interface is reporting many transmit errors.
      enabled: true
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    # NodeHighNumberConntrackEntriesUsed:
    #   annotations:
    #     description: '{{  $value | humanizePercentage  }} of conntrack entries are used.'
    #     summary: Number of conntrack are getting close to the limit.
    #   enabled: true
    #   expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
    #   labels:
    #     severity: warning
    #     controlPlane: true
    NodeTextFileCollectorScrapeError:
      annotations:
        description: Node Exporter text file collector failed to scrape.
        summary: Node Exporter text file collector failed to scrape.
      enabled: true
      expr: node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
        controlPlane: true
    NodeClockSkewDetected:
      annotations:
        description: Clock on {{  $labels.instance  }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
        summary: Clock skew detected.
      enabled: true
      expr: |-
        (
          node_timex_offset_seconds > 0.05
        and
          deriv(node_timex_offset_seconds[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds < -0.05
        and
          deriv(node_timex_offset_seconds[5m]) <= 0
        )
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    NodeClockNotSynchronising:
      annotations:
        description: Clock on {{  $labels.instance  }} is not synchronising. Ensure NTP is configured on this host.
        summary: Clock not synchronising.
      enabled: true
      expr: |-
        min_over_time(node_timex_sync_status[5m]) == 0
        and
        node_timex_maxerror_seconds >= 16
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    NodeRAIDDegraded:
      annotations:
        description: RAID array '{{  $labels.device  }}' on {{  $labels.instance  }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
        summary: RAID Array is degraded
      enabled: true
      expr: node_md_disks_required - ignoring (state) (node_md_disks{state="active"}) > 0
      for: 15m
      labels:
        severity: critical
        controlPlane: true
    NodeRAIDDiskFailure:
      annotations:
        description: At least one device in RAID array on {{  $labels.instance  }} failed. Array '{{  $labels.device  }}' needs attention and possibly a disk swap.
        summary: Failed device in RAID array
      enabled: true
      expr: node_md_disks{state="fail"} > 0
      labels:
        severity: warning
        controlPlane: true
  node-network-alerts:
    NodeNetworkInterfaceFlapping:
      annotations:
        description: Network interface "{{  $labels.device  }}" changing it's up status often on node-exporter {{  $labels.namespace  }}/{{  $labels.pod  }}"
      enabled: true
      expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
        controlPlane: true
  aws-auto-scaling:
    AwsAutoScalingGroupMaxedOutCritical:
      enabled: true
      expr: (aws_autoscaling_group_total_instances_average offset 10m /aws_autoscaling_group_max_size_average offset 10m) * 100 == 100
      for: 10m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group reached the maximum available nodes
        summary: AWS Auto-Scaling Group maxed out
    AwsAutoScalingGroupMaxedOutWarning:
      enabled: true
      expr: (aws_autoscaling_group_total_instances_average offset 10m /aws_autoscaling_group_max_size_average offset 10m) * 100 > 85
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group is 85% utilized.
        summary: AWS Auto-Scaling Group getting close to the maximum
    AwsAutoScalingGroupCantScaleUp:
      enabled: true
      expr: (aws_autoscaling_group_desired_capacity_average  offset 10m - aws_autoscaling_group_total_instances_average offset 10m) > 0
      for: 10m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: AWS Auto-Scaling Group can't scale up and has not reached the maximum capacity
        summary: AWS Auto-Scaling Group can't scale up
  eks-cni:
    EksAvailableIPsPerNode:
      annotations:
        description: Instance {{ $labels.instance }} has less than 5 IPs available.
      enabled: true
      expr: sum by(instance) (awscni_total_ip_addresses) - sum by(instance) (awscni_assigned_ip_addresses)
        < 5
      for: 30m
      labels:
        severity: critical
        controlPlane: true
    EksAvailableIPsPerCluster:
      annotations:
        description: The maximum available IPs in the cluster is getting closer to 0.
      enabled: true
      expr: sum(awscni_total_ip_addresses) > 2900
        < 10
      for: 10m
      labels:
        severity: critical
        controlPlane: true
    AwsCniApiErrorsCritical:
      annotations:
        summary: The aws-cni has errors while comunicating with the AWS API, this could lead to pods not able to boot.
        description: 'Node "{{  $labels.instance  }}" has {{  $labels.value  }} errors, error is: {{  $labels.error  }}"'
      enabled: true
      expr: increase(awscni_aws_api_error_count[1m]) > 0
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    AwsCniApiErrorsWarn:
      annotations:
        summary: The aws-cni has errors while comunicating with the AWS API, this could lead to pods not able to boot.
        description: 'Node "{{  $labels.instance  }}" has {{  $labels.value  }} errors, error is: {{  $labels.error  }}"'
      enabled: true
      expr: increase(awscni_aws_api_error_count[1m]) > 0
      for: 30m
      labels:
        severity: warning
        controlPlane: true
  external-dns-alerts:
    ExternalDNSRegistryError:
      enabled: true
      expr: rate(external_dns_registry_errors_total[10m]) > 0
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: "Registry errors are mostly Provider errors, unless there's some coding flaw in the registry package. Provider errors often arise due to accessing their APIs due to network or missing cloud-provider permissions when reading records"
        summary: External-DNS has registry errors when trying to add new records
    ExternalDNSSourceError:
      enabled: true
      expr: rate(external_dns_source_errors_total[10m]) > 0
      for: 5m
      labels:
        severity: warning
        controlPlane: true
      annotations:
        description: "Sources errors are mostly Kubernetes API objects. Examples of source errors may be connection errors to the Kubernetes API server itself or missing RBAC permissions. It can also stem from incompatible configuration in the objects itself like invalid characters, processing a broken fqdnTemplate, etc"
        summary: External-DNS has source errors when trying to add new records
  spinnaker:
    SpinnakerProbeFailing:
      enabled: true
      expr: up{job="prometheus-blackbox-exporter"} == 0 or probe_success{job="prometheus-blackbox-exporter", target="spinnaker-api"} == 0
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Spinnaker API endpoint doesn't pass the healthcheck during last 5 minutes
        summary: Spinnaker API Endpoint is Down
    SpinnakerUIProbeFailing:
      enabled: true
      expr: up{job="prometheus-blackbox-exporter"} == 0 or probe_success{job="prometheus-blackbox-exporter", target="spinnaker-ui"} == 0
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Spinnaker UI endpoint doesn't pass the healthcheck during last 5 minutes
        summary: Spinnaker UI Endpoint is Down
    SpinnakerClouddriverJVMHigh:
      enabled: true
      expr: (sum(clouddriver:jvm:memory:used__value) by (instance, area) / sum(clouddriver:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Clouddriver memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Clouddriver) JVM Memory Usage is too High
    SpinnakerIgorJVMHigh:
      enabled: true
      expr: (sum(igor:jvm:memory:used__value) by (instance, area) / sum(igor:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Igor memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Igor) JVM Memory Usage is too High
    SpinnakerFront50JVMHigh:
      enabled: true
      expr: (sum(front50:jvm:memory:used__value) by (instance, area) / sum(front50:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Front50 memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Front50) JVM Memory Usage is too High
    SpinnakerEchoJVMHigh:
      enabled: true
      expr: (sum(echo:jvm:memory:used__value) by (instance, area) / sum(echo:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Echo memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Echo) JVM Memory Usage is too High
    SpinnakerRoscoJVMHigh:
      enabled: true
      expr: (sum(rosco:jvm:memory:used__value) by (instance, area) / sum(rosco:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Rosco memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Rosco) JVM Memory Usage is too High
    SpinnakerGateJVMHigh:
      enabled: true
      expr: (sum(gate:jvm:memory:used__value) by (instance, area) / sum(gate:jvm:memory:max__value) by (instance, area)) > 0.9
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Gate memory used by JVM exceeds 90% of the available memory
        summary: Spinnaker (Gate) JVM Memory Usage is too High
    SpinnakerClouddriverDown:
      enabled: true
      expr: absent(up{service=~"spin-clouddriver"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Clouddriver pod is unreachable or not available'
        summary: Spinnaker Clouddriver is down
    SpinnakerIgorDown:
      enabled: true
      expr: absent(up{service=~"spin-igor"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Igor pod is unreachable or not available'
        summary: Spinnaker Igor is down
    SpinnakerFront50Down:
      enabled: true
      expr: absent(up{service=~"spin-front50"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Front50 pod is unreachable or not available'
        summary: Spinnaker Front50 is down
    SpinnakerOrcaDown:
      enabled: true
      expr: absent(up{service=~"spin-orca"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Orca pod is unreachable or not available'
        summary: Spinnaker Orca is down
    SpinnakerEchoDown:
      enabled: true
      expr: absent(up{service=~"spin-echo"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Echo pod is unreachable or not available'
        summary: Spinnaker Echo is down
    SpinnakerRoscoDown:
      enabled: true
      expr: absent(up{service=~"spin-rosco"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Rosco pod is unreachable or not available'
        summary: Spinnaker Rosco is down
    SpinnakerGateDown:
      enabled: true
      expr: absent(up{service=~"spin-gate"}) == 1
      for: 5m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: 'Gate pod is unreachable or not available'
        summary: Spinnaker Gate is down
  aws-nlb-alerts:
    LessThan4HealthyHosts:
      enabled: true
      expr: sum(aws_networkelb_healthy_host_count_minimum) by (load_balancer,target_group) < 4
      for: 40m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Less then 4 healthy hosts in Target group
        summary:  We are seeing last healthy hosts (target groups) in the last 40 minutes than before
    UnhealthyHostsNLB:
      enabled: true
      expr: sum(aws_networkelb_un_healthy_host_count_maximum) by (load_balancer,target_group) > 0
      for: 40m
      labels:
        severity: critical
        controlPlane: true
      annotations:
        description: Unhealthy Hosts in the NLB
        summary:  We see unhealthy hosts in the NLB for over 40 Minutes
  argo-rollouts:
    ArgoRolloutDeploymentError:
      enabled: true
      expr: |
        label_replace(
          rollout_info{phase="Error"},
          "namespace","$1","exported_namespace", "(.+)"
          ) > 0
      annotations:
        description: "ArgoRollout of {{ $labels.name }} holds an Error status for its deployment status"
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    ArgoRolloutInvalidSpec:
      enabled: true
      expr: |
        label_replace(
          rollout_info{phase="InvalidSpec"},
          "namespace","$1","exported_namespace", "(.+)"
          ) > 0
      annotations:
        description: "ArgoRollout of {{ $labels.name }} has an invalid rollout spec"
      for: 5m
      labels:
        severity: warning
        controlPlane: true
    ArgoRolloutQueueDepth:
      enabled: true
      expr: sum(workqueue_depth{name="Rollouts"}) > 5
      annotations:
        description: "Argo rollout holds a queue of more 5 rollouts for more than 30 minutes"
      for: 30m
      labels:
        severity: critical
        controlPlane: true
  telegraf:
    TelegrafWriteErrors:
      enabled: true
      expr: increase(internal_write_errors[5m]) > 0
      annotations:
        description: "Telegraf has write errors (to outputs) please check telegraf logs"
      for: 10m
      labels:
        severity: warning
        controlPlane: true
  deamonsetcapacity:
    DaemonSetCapacityCPU:
      enabled: true
      expr: sum(kube_pod_container_resource_requests{resource=~"cpu",pod=~".*aws.*|.*ebs.*|.*filebeat.*|kafka-backup.*|.*prometheus-node-exporter.*|kube-proxy.*|istio-cni.*|"}) by (node) / avg(eagle_node_resource_allocatable_cpu_cores) by (node) * 100 > 50
      annotations:
        description: "daemonSet CPU allocations are over 50%, please consider bigger instance types "
      for: 0m
      labels:
        severity: warning
        controlPlane: true
    DaemonSetCapacityMemory:
      enabled: true
      expr: sum(kube_pod_container_resource_requests{resource=~"memory",pod=~".*aws.*|.*ebs.*|.*filebeat.*|kafka-backup.*|.*prometheus-node-exporter.*|kube-proxy.*|istio-cni.*|"}) by (node) /  avg(eagle_node_resource_allocatable_memory_bytes) by (node) * 100 > 50
      annotations:
        description: "daemonSet Memory allocations are over 50%, please consider bigger instance type"
      for: 0m
      labels:
        severity: warning
        controlPlane: true
  cert-manager:
    CertManagerAbsent:
      enabled: true
      expr: absent(up{job="cert-manager"})
      annotations:
        summary: "Cert Manager has dissapeared from Prometheus service discovery."
        description: "New certificates will not be able to be minted, and existing ones can't be renewed until cert-manager is back."
      for: 10m
      labels:
        severity: warning
        controlPlane: true
    CertManagerCertExpirySoon:
      enabled: true
      expr: |-
        avg by (exported_namespace, namespace, name) (
          certmanager_certificate_expiration_timestamp_seconds - time()
        ) < (21 * 24 * 3600) # 21 days in seconds
      annotations:
        summary: "The cert `{{ $labels.name }}` is {{ $value | humanizeDuration }} from expiry, it should have renewed over a week ago."
        description: "The domain that this cert covers will be unavailable after {{ $value | humanizeDuration }}. Clients using endpoints that this cert protects will start to fail in {{ $value | humanizeDuration }}."
      for: 1h
      labels:
        severity: warning
        controlPlane: true
    CertManagerCertExpiryVerySoon:
      enabled: true
      expr: |-
        avg by (exported_namespace, namespace, name) (
          certmanager_certificate_expiration_timestamp_seconds - time()
        ) < (5 * 24 * 3600) # 5 days in seconds
      annotations:
        summary: "The cert `{{ $labels.name }}` is {{ $value | humanizeDuration }} from expiry, it should have renewed over a week ago."
        description: "The domain that this cert covers will be unavailable after {{ $value | humanizeDuration }}. Clients using endpoints that this cert protects will start to fail in {{ $value | humanizeDuration }}."
      for: 1h
      labels:
        severity: critical
        controlPlane: true
    CertManagerCertNotReady:
      enabled: true
      expr: |-
            max by (name, exported_namespace, namespace, condition) (
              certmanager_certificate_ready_status{condition!="True"} == 1
            )
      annotations:
        summary: "The cert `{{ $labels.name }}` is not ready to serve traffic."
        description: "This certificate has not been ready to serve traffic for at least 10m. If the cert is being renewed or there is another valid cert, the ingress controller _may_ be able to serve that instead."
      for: 10m
      labels:
        severity: critical
        controlPlane: true
    CertManagerHittingRateLimits:
      enabled: true
      expr: |-
            sum by (host) (
              rate(certmanager_http_acme_client_request_count{status="429"}[5m])
            ) > 0
      annotations:
        summary: "Cert manager hitting LetsEncrypt rate limits."
        description: "Depending on the rate limit, cert-manager may be unable to generate certificates for up to a week."
      for: 5m
      labels:
        severity: warning
        controlPlane: true
  elasticsearch-alerts:
    ElasticsearchHeapUsageTooHigh:
      enabled: false
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: Elasticsearch Heap Usage Too High (instance {{ $labels.instance }})
        description: "The heap usage is over 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchHeapUsageWarning:
      enabled: false
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
      for: 20m
      labels:
        severity: warning
      annotations:
        description: "The heap usage is over 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        summary: Elasticsearch Heap Usage warning (instance {{ $labels.instance }})
    ElasticsearchDiskOutOfSpace:
      enabled: false
      expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 15
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: Elasticsearch disk out of space (instance {{ $labels.instance }})
        description: "The disk usage is over 85%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchDiskSpaceLow:
      enabled: false
      expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20
      for: 20m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch disk space low (instance {{ $labels.instance }})
        description: "The disk usage is over 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchClusterRed:
      enabled: false
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: Elasticsearch Cluster Red (instance {{ $labels.instance }})
        description: "Elastic Cluster Red status\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchClusterYellow:
      enabled: false
      expr: elasticsearch_cluster_health_status{color="yellow"} == 1
      for: 60m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch Cluster Yellow (instance {{ $labels.instance }})
        description: "Elastic Cluster Yellow status\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchRelocatingShardsTooLong:
      enabled: false
      expr: elasticsearch_cluster_health_relocating_shards > 0
      for: 120m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch relocating shards too long (instance {{ $labels.instance }})
        description: "Elasticsearch has been relocating shards for 60min\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchInitializingShardsTooLong:
      enabled: false
      expr: elasticsearch_cluster_health_initializing_shards > 0
      for: 60m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch initializing shards too long (instance {{ $labels.instance }})
        description: "Elasticsearch has been initializing shards for 60 min\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchUnassignedShards:
      enabled: false
      expr: elasticsearch_cluster_health_unassigned_shards > 0
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch unassigned shards (instance {{ $labels.instance }})
        description: "Elasticsearch has unassigned shards\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElasticsearchPendingTasks:
      enabled: false
      expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch pending tasks (instance {{ $labels.instance }})
        description: "Elasticsearch has pending tasks. Cluster works slowly.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    ElastisearchPostJobFailed:
      enabled: false
      expr: kube_job_status_failed{job_name=~"elastisearch-post-install-job.*"} > 0
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: Elasticsearch post-install-job Job failed (instance {{ $labels.instance }})
        description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    ElastisearchClusterUnavilable:
      enabled: false
      expr: absent(sum(elasticsearch_cluster_health_status))
      for: 30m
      labels:
        severity: critical
      annotations:
        summary: Elasticsearch cluster health is unavailable
        description: "Cluster {{ $labels.cluster }} is unavailable"
  keda:
    KedaScalerErrors:
      enabled: true
      expr: rate(keda_metrics_adapter_scaler_errors_total[5m]) > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: 'KEDA Scalers return errors'
        description: 'KEDA uses Scalers to scale up/down, there is an error please check keda operator logs'
  karpenter:
    KarpenterPendingPods:
      enabled: true
      expr: sum(karpenter_pods_state{phase="Pending"}) > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: 'Karpenter has pending pods, please check why it happens'
        description: 'Karpenter should allocate nodes for all pods, if some pod is pending for over 15 minutes, it means there is a an issue.'
    KarpenterNoCloudProviderCalls:
      enabled: true
      expr: sum(delta(karpenter_cloudprovider_duration_seconds_sum[1m])) == 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: 'Karpenter has stopped sampling AWS, please check it'
        description: 'Karpenter should always sample AWS for prices and updates, please check logs of karpenter.'
  k8s-workload-alerts:
# Pod Alerts
    K8SPodContainerCrashLooping:
      enabled: true
      expr: rate(kube_pod_container_status_restarts_total[5m]) * 1200 > 0
      for: 20m
      labels:
        severity: warning
      annotations:
        description: "Pod container is in crash loop for longer than 20 minutes"
        summary: "This means that one of the containers in the pod has exited unexpectedly, and perhaps with a non-zero error code even after restarting."
        link: "https://komodor.com/learn/how-to-fix-crashloopbackoff-kubernetes-error/"
    K8SPodContainerErrImagePull:
      enabled: true
      expr: sum(sum_over_time(kube_pod_container_status_waiting_reason{reason="ErrImagePull"}[1m]) > 0) by (container)
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason }} for longer than 15m."
        summary: "General image pull error"
    K8SPodContainerImagePullBackOff:
      enabled: true
      expr: sum(sum_over_time(kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}[1m]) > 0) by (container)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 30m."
        summary: "Container image pull failed, kubelet is backing off image pull"
        link: "https://www.tutorialworks.com/kubernetes-imagepullbackoff/"
    K8SPodContainerCreatingStuck:
      enabled: true
      expr: sum(sum_over_time(kube_pod_container_status_waiting_reason{reason="ContainerCreating"}[1m]) > 0) by (container)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 30m."
        summary: "Container is stuck in ContainerCreating"
        link: "https://serverfault.com/questions/728727/kubernetes-stuck-on-containercreating"
    KubernetesPodCrashLooping:
      enabled: true
      expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod restarts more than 3 times in a minute looping (instance {{ $labels.instance }})"
        description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    K8SPodContainerCrashLoopBackOff:
      enabled: true
      expr: sum(sum_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}[1m]) > 0) by (container, namespace)
      for: 20m
      labels:
        severity: critical
      annotations:
        description: "Pod container has an {{ $labels.reason}} for longer than 20m."
        summary: "Container Terminated and Kubelet is backing off the restart"
        link: "https://komodor.com/learn/how-to-fix-crashloopbackoff-kubernetes-error/"
    K8SPodContainerCPURequests:
      enabled: true
      expr: label_join(rate(container_cpu_user_seconds_total{namespace!="kube-system",container!="POD",container!=""}[5m]), "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container) group_left label_join(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system",container!=""}, "pod_container", "/" , "pod", "container") >= 150
      for: 15m
      labels:
        severity: warning
      annotations:
        description: "Pod container CPU usage has been 150% over CPU requests for longer than 15m."
        summary: "Pod container CPU requests should be increased"
    K8SPodContainerCPULimits:
      enabled: true
      expr: label_join(rate(container_cpu_user_seconds_total{namespace!="kube-system",container!="POD",container!=""}[5m]), "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_limits_cpu_cores{namespace!="kube-system",container!=""}, "pod_container", "/" , "pod", "container") >= 90
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "Pod container CPU usage has been more ultilized than 90% of CPU limits for longer than 15m."
        summary: "Pod container CPU limits should be increased"
    K8SPodContainerMemRequests:
      enabled: true
      expr: label_join(container_memory_working_set_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") >= 150
      for: 15m
      labels:
        severity: warning
      annotations:
        description: "Pod container Mem usage has been 150% over Mem requests for longer than 15m."
        summary: "Pod container Mem requests should be increased"
    K8SPodContainerMemLimits:
      enabled: true
      expr: label_join(container_memory_working_set_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") * 100 / on (kubernetes_cluster, namespace, pod_container)  group_left label_join(kube_pod_container_resource_limits_memory_bytes{namespace!="kube-system",container!="POD",container!=""}, "pod_container", "/" , "pod", "container") > 90
      for: 15m
      labels:
        severity: critical
      annotations:
        description: "Pod container Mem usage has been more ultilized than 90% of MEM limits for longer than 15m."
        summary: "Pod container Mem limits should be increased, you are in danger of getting OOM!"
    K8SPodPendingState:
      enabled: true
      expr: sum by (kubernetes_cluster, namespace, pod, phase) (kube_pod_status_phase{phase="Pending"}) > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        description: "Pod has been in pending state for longer than 15m."
        summary: "The Pod has been accepted by the Kubernetes system, but one or more of the Container images has not been created. This includes time before being scheduled as well as time spent downloading images over the network, which could take a while."
        link: "https://stackify.com/why-do-kubernetes-pod-stay-in-pending-state/"
    KubernetesContainerOomKiller:
      enabled: true
      expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
      for: 0m
      labels:
        severity: warning
      annotations:
        summary: Kubernetes container oom killer (instance {{ $labels.instance }})
        description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    K8SPodContainerCpuSystemSecondsTotal:
      enabled: true
      expr: rate(container_cpu_system_seconds_total{pod!=""}[5m]) > 3
      for: 10m
      labels:
        severity: warning
      annotations:
        summary:  Pod {{ $labels.pod }} in {{ $labels.namespace }} is consuming CPU too long time {{ $value }}
        description: "Total amount of system time by pod {{ $labels.pod }} is {{ $value }} the last 10 minutes"
# Deployment Alerts
    DeploymentNotAvailable40Percent:
      enabled: true
      expr: (kube_deployment_status_replicas_unavailable / kube_deployment_status_replicas * 100 > 40) and (kube_deployment_status_replicas > 2)
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Deployment has 40% unavailable replicas on {{  $labels.deployment  }}"
        description: "please check why {{ $labels.deployment }} has 40% unavailable replicas"
    K8SDeploymentAvailableReplicasMismatch:
      enabled: true
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "Deployment has not the expected number of available replicas for longer than 30m"
        summary: "Check out why the current replicas are less then what the specs says there should be "
    K8SDeploymentUpdatedReplicasMismatch:
      enabled: true
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_updated
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "Deployment has not the expected number of updated replicas for longer than 30m."
        summary: "Check out why the current replicas are less then what the specs says there should be"
    K8SDeploymentPending:
      enabled: true
      expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
      for: 60m
      labels:
        severity: warning
      annotations:
        description: "Deployment generation does not match for longer than 60m"
        summary: "Deployment has failed but has not been rolled back"
# Statefulset alerts
    K8SStatefulSetAvailableReplicasMismatch:
      enabled: true
      expr: kube_statefulset_status_replicas != kube_statefulset_status_replicas_ready
      for: 60m
      labels:
        severity: warning
      annotations:
        description: "StatefulSet has not the expected number of ready replicas for longer than 60m"
    K8SStatefulSetPending:
      enabled: true
      expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
      for: 60m
      labels:
        severity: warning
      annotations:
        description: "StatefulSet generation does not match for longer than 60m"

# DaemonSet Alerts
    K8SDaemonSetAvailablePodsMismatch:
      enabled: true
      expr: kube_daemonset_status_number_ready != kube_daemonset_status_desired_number_scheduled
      for: 1h
      labels:
        severity: warning
      annotations:
        description: "DaemonSet has not the expected number of ready replicas for longer than 1h."

# HPA Alerts
    KubernetesHpaScalingAbility:
      enabled: true
      expr: kube_hpa_status_condition{status="false", condition="AbleToScale"} == 1
      for: 20m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes HPA can't scale additional pods (instance {{ $labels.instance }})"
        description: "HPA is unable to scale more pods(even though maximum wasn't reached)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    KubernetesHpaScaleCapability:
      enabled: true
      expr: kube_hpa_status_desired_replicas >= kube_hpa_spec_max_replicas
      for: 20m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes HPA scale capability (instance {{ $labels.instance }})"
        description: "The maximum number of desired Pods has been hit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
# CronJob Alerts - we need more here
    CronJobStatusFailed:
      enabled: true
      expr: |
        job_cronjob:kube_job_status_failed:sum
        * ON(label_name) GROUP_RIGHT()
        kube_cronjob_labels
        > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        description: '{{ $labels.cronjob }} last run has failed {{ $value }} times.'
    KubernetesCronjobDidntRun:
      enabled: true
      expr: |
        max by(namespace, cronjob) (
          kube_cronjob_next_schedule_time -
          kube_cronjob_status_last_schedule_time
        ) < max without(owner_name) (
          label_replace(
            round(
              (time() - max by (namespace, owner_name) (
                  kube_job_status_start_time *
                    on (job_name) group_left(owner_name)
                    max by (namespace, owner_name, job_name) (
                      kube_job_owner{owner_kind="CronJob"}))
              ) * 0.85 - 60
            )
          , "cronjob", "$1", "owner_name", "(.*)")
        ) unless max by (namespace,cronjob) (kube_cronjob_status_active)
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes CronJob haven't run within their (last period + 15%) seconds. (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} haven't run within their (last period + 15%) seconds.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
# ReplicaSet Alerts
    KubernetesReplicassetMismatch:
      enabled: true
      expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
        description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  kubernetes-storage-alerts:
    KubernetesPersistentvolumeclaimPending:
      enabled: true
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    K8SPersistentVolumeRunningFull24H:
      enabled: true
      expr: (kubelet:persistent_volume_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "PersistentVolume will be full within the next 24 hours."
    K8SPersistentVolumeRunningFull2H:
      enabled: true
      expr: (kubelet:persistent_volume_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_avail:ratio[6h], 3600 * 2) < 0)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume will be full within the next 2 hours."
    K8SPersistentVolumeInodesRunningFull24H:
      enabled: true
      expr: (kubelet:persistent_volume_inode_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_inode_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "PersistentVolume inode will reached limit within the next 24 hours."
    K8SPersistentVolumeInodesRunningFull2H:
      enabled: true
      expr: (kubelet:persistent_volume_inode_usage:ratio > 0.85) and (predict_linear(kubelet:persistent_volume_inode_avail:ratio[6h], 3600 * 24) < 0)
      for: 30m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume inode will reached limit within the next 2 hours."
    K8SPersistentVolumePendingState:
      enabled: true
      expr: kube_persistentvolume_status_phase{job=~"kubernetes_.+", phase="Pending"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume has been in pending state for longer than 10m."
    K8SPersistentVolumeFailedState:
      enabled: true
      expr: kube_persistentvolume_status_phase{job=~"kubernetes_.+", phase="Failed"} > 0
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "PersistentVolume has been in failed state for longer than 10m."
    KubePersistentVolumeErrors:
      annotations:
        description: The persistent volume {{  $labels.persistentvolume  }} has status {{  $labels.phase  }}.
        summary: PersistentVolume is having issues with provisioning.
      enabled: true
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: critical
    KubernetesVolumeOutOfDiskSpace:
      enabled: true
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
        description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  telegraf:
    TelegrafStatsdDroppedPackets:
      enabled: false
      expr: sum(increase(internal_statsd_udp_packets_dropped[15m])) by (app,namespace) > 0
      annotations:
        description: "Telegarf agent dropping metrics. It means that the service sends a high amount of metrics that telegraf agent can't handle. Please increase one of the following: CPU Cores/Memory/allowed_pending_messages"
      for: 0m
      labels:
        severity: warning
    TelegrafDroppedMetrics:
      enabled: true
      expr: sum(increase(internal_agent_metrics_dropped[15m])) by (app, namespace) > 0
      annotations:
        description: "Telegraf is dropping metrics, please check telegraf logs"
      for: 0m
      labels:
        severity: warning

  rollout:
    RolloutNotAvailable40Percent:
      enabled: true
      expr: |
        (label_replace(
          rollout_info_replicas_unavailable,
          "namespace","$1","exported_namespace", "(.+)"
          )
        /
        label_replace(
          rollout_info_replicas_desired,
          "namespace","$1","exported_namespace", "(.+)"
          ) * 100 > 40) and (rollout_info_replicas_desired > 2)
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Rollout has 40% unavailable replicas on {{  $labels.name  }}"
        description: "please check why {{ $labels.name }} has 40% unavailable replicas"
    RolloutAvailableReplicasMismatch:
      enabled: true
      expr: |
        label_replace(
          rollout_info_replicas_desired,
          "namespace","$1","exported_namespace", "(.+)"
          )
        !=
        label_replace(
          rollout_info_replicas_available,
          "namespace","$1","exported_namespace", "(.+)"
          )
      for: 30m
      labels:
        severity: warning
      annotations:
        description: "Rollout has not the expected number of available replicas for longer than 30m"
        summary: "Check out why the current replicas are less then what the specs says there should be"
    ArgoRolloutDeploymentStuckOnPaused:
      enabled: true
      expr: |
        label_replace(
          rollout_info{phase="Paused"},
          "namespace","$1","exported_namespace", "(.+)"
          ) > 0
      annotations:
        description: "There is a paused Rollout for more than 1 days"
        summary: "Check out why the deployment was paused"
      for: 1d
      labels:
        severity: warning
